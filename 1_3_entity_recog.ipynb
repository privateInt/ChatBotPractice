{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "## Entity recognition 테스크를 위한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "'''\n",
    "pip install pytorch-crf\n",
    "https://pytorch-crf.readthedocs.io/en/stable/\n",
    "'''\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed, save\n",
    "'''\n",
    " PyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\n",
    " 그 외에 추가로 제공하고 싶은게 있으면 아래와 같이 커스텀이 가능합니다.\n",
    " 여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\n",
    "'''\n",
    "class EntityDataset(data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor, lengths):\n",
    "        super(EntityDataset, self).__init__()\n",
    "\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        self.lengths = lengths\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.lengths[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.entity_label_dir = \"./data/dataset/entity_label.json\"\n",
    "        self.entity_data_dir = \"./data/dataset/entity_data.csv\"\n",
    "        \n",
    "        self.entity_label = self.load_entity_label()\n",
    "        self.prep = Preprocessing()\n",
    "        \n",
    "    def load_entity_label(self):\n",
    "        f = open(self.entity_label_dir, encoding=\"UTF-8\")\n",
    "        entity_label = json.loads(f.read())\n",
    "        self.entitys = list(entity_label.keys())\n",
    "        return entity_label\n",
    "    \n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "    \n",
    "    def make_entity_dataset(self, embed):\n",
    "        entity_dataset = pd.read_csv(self.entity_data_dir)\n",
    "        entity_querys = self.tokenize_dataset(entity_dataset[\"question\"].tolist())\n",
    "        labels = []\n",
    "        for label in entity_dataset[\"label\"].to_list():\n",
    "            temp = []\n",
    "            for entity in label.split():\n",
    "                temp.append(self.entity_label[entity])\n",
    "            labels.append(temp)\n",
    "        dataset = list(zip(entity_querys, labels))\n",
    "        entity_train_dataset, entity_test_dataset = self.word2idx_dataset(dataset, embed)\n",
    "        return entity_train_dataset, entity_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list, lengths = [], [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset)\n",
    "        for query, label in dataset :\n",
    "            q_vec = embed.query2idx(query)\n",
    "            lengths.append(len(q_vec))\n",
    "            \n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec)\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "\n",
    "            label = self.prep.pad_idx_sequencing(label)\n",
    "            label_list.append(label)\n",
    "            flag = False\n",
    "\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.tensor(label_list)\n",
    "\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len*train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "\n",
    "            train_length = lengths[:train_size]\n",
    "            test_length = lengths[train_size+1:]\n",
    "\n",
    "            train_dataset = EntityDataset(train_x,train_y,train_length)\n",
    "            test_dataset = EntityDataset(test_x,test_y,test_length)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR x!=y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-DATE': 1,\n",
       " 'B-LOCATION': 2,\n",
       " 'B-PLACE': 3,\n",
       " 'B-RESTAURANT': 4,\n",
       " 'E-DATE': 5,\n",
       " 'E-LOCATION': 6,\n",
       " 'E-PLACE': 7,\n",
       " 'E-RESTAURANT': 8,\n",
       " 'I-DATE': 9,\n",
       " 'I-RESTAURANT': 10,\n",
       " 'S-DATE': 11,\n",
       " 'S-LOCATION': 12,\n",
       " 'S-PLACE': 13,\n",
       " 'S-RESTAURANT': 14,\n",
       " '<START_TAG>': 15,\n",
       " '<STOP_TAG>': 16}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inside, Out, Begin, End, Single\n",
    "#IO :  TAG라면 I 을 아니면 O 로 태그.\n",
    "#BIO : TAG의 길이가 2이상이면 첫 번째 단어는 B를 붙이고 그 뒤의 단어들은 I를 붙인다.\n",
    "#BIOES : BIO에서 단어의 길이가 3이상인 단어는 마지막 단어에 E를 붙인다. 그리고 단어의 길이가 1이라면, S를 붙인다.\n",
    "# S : 단독\n",
    "# B : 복합의 시작 (단독 사용 불가)\n",
    "# I : 복합의 중간 (단독 사용 불가)\n",
    "# E : 복합의 끝  (단독 사용 불가)\n",
    "# O : 의미 없음\n",
    "dataset.entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question    label\n",
       "0      야 먼지 알려주겠니    O O O\n",
       "1  아니 먼지 정보 알려주세요  O O O O\n",
       "2    그 때 미세먼지 어떨까  O O O O\n",
       "3     그 때 먼지 좋으려나  O O O O\n",
       "4   미세먼지 어떨 것 같은데  O O O O"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset = pd.read_csv(dataset.entity_data_dir)\n",
    "\n",
    "entity_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE B-LOCATION E-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O B-LOCATION E-LOCATION O O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O O</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-RESTAURANT O</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-RESTAURANT O O</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question\n",
       "label                                                  \n",
       "B-DATE E-DATE B-LOCATION E-LOCATION O O               4\n",
       "B-DATE E-DATE O                                       6\n",
       "B-DATE E-DATE O B-LOCATION E-LOCATION O O O O         1\n",
       "B-DATE E-DATE O O                                    42\n",
       "B-DATE E-DATE O O O                                  31\n",
       "...                                                 ...\n",
       "S-RESTAURANT O S-LOCATION O O                         4\n",
       "S-RESTAURANT O S-RESTAURANT O                         3\n",
       "S-RESTAURANT S-LOCATION O                             1\n",
       "S-RESTAURANT S-LOCATION O O O                         1\n",
       "S-RESTAURANT S-RESTAURANT O O                         2\n",
       "\n",
       "[354 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset.groupby(['label']).count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM-CRF Models for Sequence Tagging\n",
    "## * Zhiheng Huang,Wei Xu,Kai Yu\n",
    "### tensorflow code : https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF/blob/main/model.py\n",
    "### tensorflow, keras code : https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, w2v, tag_to_ix, hidden_dim, batch_size):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = w2v.size()[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size =  w2v.size()[0]\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.batch_size = batch_size\n",
    "        self.START_TAG = \"<START_TAG>\"\n",
    "        self.STOP_TAG = \"<STOP_TAG>\"\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(self.vocab_size+2, self.embedding_dim)\n",
    "        self.word_embeds.weight[2:].data.copy_(w2v)\n",
    "        #self.word_embeds.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM 파라미터 정의\n",
    "        # bidirectional : 양방향 LSTM\n",
    "        # num_layers    : layer의 수\n",
    "        # batch_first   : pytorch에서 LSTM 입력의 기본값은 (Length,batch,Hidden) 순서 이므로 (batch,Length,Hidden)로 바꿔주기 위함\n",
    "        # nn.LSTM(input_size, hidden_size, batch_first, num_layers)\n",
    "        # hidden_size = hidden_dim // 2 인 이유는 bidirectional = True 이기 떄문입니다.\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim // 2, batch_first=True, num_layers=1, bidirectional=True)\n",
    "    \n",
    "        # LSTM의 출력을 태그 공간으로 대응시킵니다.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "    \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # 출력층의 규칙학습을 위한 CRF 세팅\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "        \n",
    "    def init_hidden(self):#(h,c)\n",
    "        return (torch.randn(2, self.batch_size, self.hidden_dim // 2),\n",
    "                torch.randn(2, self.batch_size, self.hidden_dim // 2))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Bi-LSTM으로부터 배출 점수를 얻습니다.\n",
    "        self.batch_size = sentence.size()[0]\n",
    "        self.hidden = self.init_hidden()\n",
    "        #(2,128,128),(2,128,128)\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        #(128,20,300)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        #(128,20,256),((2,128,128),(2,128,128))\n",
    "        lstm_feats = self.hidden2tag(lstm_out)#(batch, length,tagset_size)\n",
    "        #(128,20,17)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def decode(self, logits, mask):\n",
    "        \"\"\"\n",
    "        Viterbi Decoding의 구현체입니다.\n",
    "        CRF 레이어의 출력을 prediction으로 변형합니다.\n",
    "        :param logits: 모델의 출력 (로짓)\n",
    "        :param mask: 마스킹 벡터\n",
    "        :return: 모델의 예측 (prediction)\n",
    "        \n",
    "        각 단어의 자리마다\n",
    "          word 1의 태그 확률        |  word2의 태그 확률\n",
    "         'O': 확률0,              | 'O': 확률A,\n",
    "         'B-DATE': 확률1,         | 'B-DATE': 확률B\n",
    "         'B-LOCATION': 확률2,     | 'B-LOCATION': 확률C,\n",
    "         'B-PLACE': 확률3,        | 'B-PLACE': 확률D,  \n",
    "         'B-RESTAURANT': 확률4,   | 'B-RESTAURANT': 확률E,  \n",
    "         'E-DATE': 확률5,         | 'E-DATE': 확률F,   \n",
    "         'E-LOCATION': 확률6,     | 'E-LOCATION': 확률G, \n",
    "         'E-PLACE': 확률7,        | 'E-PLACE': 확률H,  \n",
    "         'E-RESTAURANT': 확률8,   | 'E-RESTAURANT': 확률I, \n",
    "         'I-DATE': 확률9,         | 'I-DATE': 확률J,    \n",
    "         'I-RESTAURANT': 확률10,  | 'I-RESTAURANT': 확률K,\n",
    "         'S-DATE': 확률11,        | 'S-DATE': 확률L,      \n",
    "         'S-LOCATION': 확률12,    | 'S-LOCATION': 확률M,\n",
    "         'S-PLACE': 확률13,       | 'S-PLACE': 확률N,  \n",
    "         'S-RESTAURANT': 확률14,  | 'S-RESTAURANT': 확률O,\n",
    "         '<START_TAG>': 확률15,   | '<START_TAG>': 확률P, \n",
    "         '<STOP_TAG>': 확률15,    | '<STOP_TAG>': 확률Q,\n",
    "         \n",
    "         각각의 높은 확률을 뽑는 것은 보통의 딥러닝 방식으로 B단독이나 I단독, E단독같은 문제를 야기할 수 있습니다.\n",
    "         태그들의 확률 값을 받아서 \n",
    "         CRF는 태그들의 의존성을 학습할수 있어서 태그 시퀀스의 확률이 가장 높은 확률을 가지는 예측 시퀀스를 선택한다.\n",
    "         그래서 B단독이나 I단독, E단독과 같은 문제를 없애줍니다.\n",
    "         예를 들어 B-DATE, O 와 같은걸 출력하지 않습니다. (CRF는 S-DATE, O 라고 출력합니다.)\n",
    "        \"\"\"\n",
    "\n",
    "        return self.crf.decode(logits, mask)\n",
    "    \n",
    "    def compute_loss(self, label, logits, mask):\n",
    "        \"\"\"\n",
    "        학습을 위한 total loss를 계산합니다.\n",
    "        :param label: label\n",
    "        :param logits: logits\n",
    "        :param mask: mask vector\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "\n",
    "        log_likelihood = self.crf(logits, label, mask=mask, reduction='mean')\n",
    "        return - log_likelihood  # Negative log likelihood loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "entity_train_dataset, entity_test_dataset = dataset.make_entity_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(entity_train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(entity_test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[233,   2, 765,  ...,   0,   0,   0],\n",
       "        [399,   2, 488,  ...,   0,   0,   0],\n",
       "        [322, 192,   0,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [239,  53, 117,  ...,   0,   0,   0],\n",
       "        [  2, 233, 678,  ...,   0,   0,   0],\n",
       "        [429, 109,  10,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_train_dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [11,  0,  0,  ...,  0,  0,  0],\n",
       "        [12,  0,  0,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0, 13,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [12,  0,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(1481, 300)\n",
       "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=17, bias=True)\n",
       "  (crf): CRF(num_tags=17)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = embed.word2vec.wv.vectors\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "model = BiLSTM_CRF(weights, dataset.entity_label, 256, 128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [00:25<00:00,  4.89batch/s, loss=2.56]\n",
      "Epoch 0: 100%|██████████| 32/32 [00:02<00:00, 12.97batch/s, accuracy=51.1, loss=2.54]\n",
      "Epoch 1:  38%|███▊      | 48/125 [00:19<00:31,  2.42batch/s, loss=0.835]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3c9c64110918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "prev_acc = 0\n",
    "save_dir = \"./data/pretraining/1_entity_recog_model/\"\n",
    "save_prefix = \"entity_recog\"\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            y = data[1]\n",
    "            length = data[2]\n",
    "            \n",
    "            logits = model.forward(x)\n",
    "            # padding 된 부분을 마스킹하기위한 코드\n",
    "            # 우리는 length값이 존재하여 length값을 이용해서 마스크를 생성해도 가능\n",
    "            # 하지만 코드 간략화를 위해 pytorch에 where 함수를 이용해 마스크 생성\n",
    "            # torch.where 함수 설명 : https://runebook.dev/ko/docs/pytorch/generated/torch.where\n",
    "            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "            loss = model.compute_loss(y,logits,mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            \n",
    "    model.eval()\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            y = data[1]\n",
    "            length = data[2]\n",
    "            \n",
    "            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "            logits = model.forward(x)\n",
    "            \n",
    "            predicts = model.decode(logits,mask)\n",
    "            # decode함수를 통해 정확도 계산\n",
    "            corrects = []\n",
    "            for target, leng, predict in zip(y, length, predicts):\n",
    "                corrects.append(target[:leng].tolist() == predict)\n",
    "\n",
    "            accuracy = 100.0 * sum(corrects)/len(corrects)\n",
    "            accuarcy_list.append(accuracy)\n",
    "            \n",
    "            loss = model.compute_loss(y,logits,mask)\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "            \n",
    "        acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "        if(acc>prev_acc):\n",
    "            prev_acc = acc\n",
    "            save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(1481, 300)\n",
       "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=17, bias=True)\n",
       "  (crf): CRF(num_tags=17)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./data/pretraining/save/1_entity_recog_model/entity_recog_97.192_steps_7.pt\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : 이번 , 태그 : B-DATE\n",
      "단어 : 주 , 태그 : E-DATE\n",
      "단어 : 날씨 , 태그 : O\n",
      "CPU times: user 7.08 ms, sys: 2.93 ms, total: 10 ms\n",
      "Wall time: 7.07 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = \"이번 주 날씨\"\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "\n",
    "predict = model.decode(f,mask.view(1,-1))\n",
    "\n",
    "# S : 단독\n",
    "# B : 복합의 시작\n",
    "# I : 복합의 중간\n",
    "# E : 복합의 끝\n",
    "tag = [dataset.entitys[p] for p in predict[0]]\n",
    "for i, j in zip(q.split(' '),tag):\n",
    "    print(\"단어 : \"+i+\" , \"+\"태그 : \"+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : 나 , 태그 : O\n",
      "단어 : 내일 , 태그 : S-DATE\n",
      "단어 : 제주도 , 태그 : S-LOCATION\n",
      "단어 : 여행 , 태그 : O\n",
      "단어 : 가는데 , 태그 : O\n",
      "단어 : 미세먼지 , 태그 : O\n",
      "단어 : 알려줘 , 태그 : O\n",
      "CPU times: user 9.67 ms, sys: 2.53 ms, total: 12.2 ms\n",
      "Wall time: 12.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = \"나 내일 제주도 여행 가는데 미세먼지 알려줘\"\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "\n",
    "predict = model.decode(f,mask.view(1,-1))\n",
    "\n",
    "# S : 단독\n",
    "# B : 복합의 시작\n",
    "# I : 복합의 중간\n",
    "# E : 복합의 끝\n",
    "tag = [dataset.entitys[p] for p in predict[0]]\n",
    "for i, j in zip(q.split(' '),tag):\n",
    "    print(\"단어 : \"+i+\" , \"+\"태그 : \"+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

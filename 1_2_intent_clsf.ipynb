{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "## Intent classification 테스크를 위한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "#pip install sentencepiece\n",
    "#pip install pytorch-crf\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed\n",
    "\n",
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intent_label_dir = \"./data/dataset/intent_label.json\"\n",
    "        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n",
    "        \n",
    "        self.intent_label = self.load_intent_label()\n",
    "        self.prep = Preprocessing()\n",
    "    \n",
    "    def load_intent_label(self):\n",
    "        ''' 미리 만들어 둔 예측해야할 intent label 로드'''\n",
    "        f = open(self.intent_label_dir, encoding=\"UTF-8\") \n",
    "        intent_label = json.loads(f.read())\n",
    "        self.intents = list(intent_label.keys())\n",
    "        return intent_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        ''' 띄어쓰기 단위로 tokenize 적용'''\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        ''' Dataset에 tokenize 적용'''\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "\n",
    "    def make_intent_dataset(self, embed):\n",
    "        ''' intent 분류를 위한 Dataset 생성'''\n",
    "        intent_dataset = pd.read_csv(self.intent_data_dir) # 데이터 로딩\n",
    "\n",
    "        labels = [self.intent_label[label] for label in intent_dataset[\"label\"].to_list()] # label \n",
    "            \n",
    "        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist()) # 사용자 발화 tokenize\n",
    "        \n",
    "        dataset = list(zip(intent_querys, labels)) # (사용자 발화, intent) 형태로 가공\n",
    "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n",
    "        return intent_train_dataset, intent_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list = [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset) #  훈련용과 검증용으로 나눌때 intent 편형이 나타나지 않도록 데이터 셔플\n",
    "        for query, label in dataset :\n",
    "            q_vec = embed.query2idx(query) # 사용자 발화 index화\n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec) # 사용자 발화 최대길이까지 padding\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            label_list.append(torch.tensor([label]))\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.cat(label_list)\n",
    "\n",
    "        # 학습용과 검증용으로 나누기\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len*train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            \n",
    "            # TensorDataset으로 감싸기\n",
    "            '''\n",
    "             PyTorch의 TensorDataset은 tensor를 감싸는 Dataset입니다.\n",
    "\n",
    "             인덱싱 방식과 길이를 정의함으로써 이것은 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공합니다.\n",
    "\n",
    "             훈련할 때 동일한 라인에서 독립 변수와 종속 변수에 쉽게 접근할 수 있습니다.\n",
    "            '''\n",
    "            train_dataset = TensorDataset(train_x,train_y)\n",
    "            test_dataset = TensorDataset(test_x,test_y)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR x!=y\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset = pd.read_csv(dataset.intent_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question label\n",
       "0      야 먼지 알려주겠니  dust\n",
       "1  아니 먼지 정보 알려주세요  dust\n",
       "2    그 때 미세먼지 어떨까  dust\n",
       "3     그 때 먼지 좋으려나  dust\n",
       "4   미세먼지 어떨 것 같은데  dust"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dust</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question\n",
       "label               \n",
       "dust            4997\n",
       "restaurant      4997\n",
       "travel          4999\n",
       "weather         4999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.groupby(['label']).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "intent_train_dataset, intent_test_dataset = dataset.make_intent_dataset(embed)\n",
    "\n",
    "# 한번의 iter당 Batch size의 x, y를 제공한다.\n",
    "train_dataloader = DataLoader(intent_train_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "test_dataloader = DataLoader(intent_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[103, 162,  31,  ...,   0,   0,   0],\n",
       "         [ 60, 104,  57,  ...,   0,   0,   0],\n",
       "         [ 14,  28,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [352,  63,  38,  ...,   0,   0,   0],\n",
       "         [ 11, 124, 180,  ...,   0,   0,   0],\n",
       "         [189, 114, 496,  ...,   0,   0,   0]]),\n",
       " tensor([1, 0, 1,  ..., 2, 1, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_train_dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification\n",
    "## * Yoon Kim, New York University\n",
    "### tensorflow code : https://github.com/SeonbeomKim/TensorFlow-TextCNN/blob/master/TextCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, dim, kernels, dropout, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        # Word2vec으로 미리 학습해둔 임베딩 적용\n",
    "        vocab_size = w2v.size()[0]\n",
    "        emb_dim = w2v.size()[1]\n",
    "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
    "        self.embed.weight[2:].data.copy_(w2v)\n",
    "        # self.embed.weight.requires_grad = False # 임베딩 레이어 학습 유무\n",
    "        \n",
    "        # 윈도우 사이즈가 다른 각각의 conv layer 를 nn.ModuleList로 저장\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n",
    "        #Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #FC layer\n",
    "        self.fc = nn.Linear(len(kernels)*dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs] # 각 사이즈 별 결과를 list로 저장, \n",
    "        #[(out_channels, conv결과 길이),...]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] # 각 사이즈별 max_pool 결과 저장\n",
    "        #[(256,1),...]\n",
    "\n",
    "        fc_x = torch.cat(pool_x, dim=1) # concat하여 fc layer의 입력 형태로 만듬\n",
    "        #(768,1)\n",
    "\n",
    "        fc_x = fc_x.squeeze(-1) # 차원 맞추기\n",
    "        #(768)\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "\n",
    "# 모델의 가중치 저장을 위한 코드\n",
    "def save(model, save_dir, save_prefix, epoch):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = embed.word2vec.wv.vectors # word2vec weight\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "num_class = len(dataset.intent_label) \n",
    "model = textCNN(weights, 256, [3,4,5], 0.5, num_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [00:46<00:00,  2.72batch/s, accuracy=99.17355, loss=0.0761]\n",
      "Epoch 0: 100%|██████████| 32/32 [00:03<00:00,  8.45batch/s, accuracy=98.8, loss=0]      \n",
      "Epoch 1:   9%|▉         | 11/125 [00:04<00:49,  2.30batch/s, accuracy=100.0, loss=0.00208] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 10\n",
    "prev_acc = 0\n",
    "save_dir = \"./data/pretraining/1_intent_clsf_model/\"\n",
    "save_prefix = \"intent_clsf\"\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train() \n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            logit = model.forward(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(logit, target) # loass function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
    "            \n",
    "    model.eval() # weight 업데이트 금지\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "\n",
    "            logit = model.forward(x)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            accuarcy_list.append(accuracy.tolist())\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "    \n",
    "    # epoch 당 검증 셋의 정확도를 계산하고 이전 정확도 보다 높으면 저장     \n",
    "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "    if(acc>prev_acc):\n",
    "        prev_acc = acc\n",
    "        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./data/pretraining/save/1_intent_clsf_model/intent_clsf_97.217_steps_33.pt\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 : 제주도 오늘 날씨 알려줘\n",
      "의도 : weather\n",
      "CPU times: user 3.01 ms, sys: 1.3 ms, total: 4.31 ms\n",
      "Wall time: 2.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = \"제주도 오늘 날씨 알려줘\"\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "intent = dataset.intents[torch.argmax(f).tolist()]\n",
    "\n",
    "print(\"발화 : \" + q)\n",
    "print(\"의도 : \" + intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
